<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Research for Policymakers - Constitutional AI Example</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="header-container">
        <div class="nav-links">
            <a href="index.html">Template</a>
            <a href="example1.html" class="active">Example 1</a>
            <a href="example2.html">Example 2</a>
            <a href="about.html">About</a>
        </div>
        <input type="text" class="search-input" placeholder="Search papers...(coming soon)">
    </div>
    
    <div class="page-title">
        <h1>Constitutional AI: Harmlessness from AI Feedback</h1>
    </div>


    <div class="container">
        <main>
            <div class="paper-container" >
                
                <section style="font-size: 15px">
                    <h2>THE SAND - METADATA</h2>
                    
                    <div class="metadata">
                        <strong class="section-title">Topic:</strong> 
                        <div>Training harmless AI by setting a list of principles</div>
                        
                        <strong class="section-title">Domain:</strong> 
                        <div><span class="tag">Alignment</span></div>
                        
                        <strong class="section-title">Subdomains:</strong> 
                        <div>
                            <span class="tag">Reinforcement Learning</span> · 
                            <span class="tag">Constitutional AI</span> ·
                            <span class="tag">Supervised Learning</span>
                        </div>
                        
                        <strong class="section-title">Policy Issues:</strong> 
                        <div>
                            <span class="tag">Harms from AI</span> · 
                            <span class="tag">AI Alignment</span> · 
                            <span class="tag">AI Supervision</span> ·
                            <span class="tag">AI Auditing</span>
                        </div>
                        
                        <strong class="section-title">Title:</strong> 
                        <div>Constitutional AI: Harmlessness from AI Feedback</div>
                        
                        <strong class="section-title">Published:</strong> 
                        <div>December 15, 2022</div>
                        
                        <strong class="section-title">Authors:</strong> 
                        <div>Various (from Anthropic)</div>
                        
                        <strong class="section-title">Link:</strong> 
                        <div><a href="https://arxiv.org/pdf/2212.08073" target="_blank">https://arxiv.org/pdf/2212.08073</a></div>
                       
                        <strong class="section-title template-field">Technical Level:</strong>
                        <div style="display: flex; align-items: center;">
                            <div class="tech-level-indicator">
                                <div class="tech-level-dot filled"></div>
                                <div class="tech-level-dot filled"></div>
                                <div class="tech-level-dot filled"></div>
                                <div class="tech-level-dot"></div>
                                <div class="tech-level-dot"></div>
                            </div>
                        </div>
                    </div>
                </section>
                
                <section>
                    <h2>THE GIST</h2>
                    <h3 class="section-title">So... What? (Policy Bottom line)</h3>
                    <p>Constitutional AI (giving an AI a set of principles to abide by) is shown to be able to train AI systems to be (mostly) harmless without the need for humans to label harmful responses, by using a set of principles and directing the AI to self-critique. This makes it a scalable approach to AI safety while making the principles the AI follows explicit, potentially enabling more effective governance through transparency and auditability, rather than being restricted to evaluating the AIs output. 
                    This approach is currently used by Anthropic to align their frontier model Claude.
                </p>
                    
                                        
                    <h3 class="section-title">Governance considerations</h3>
                    <ul>
                        <li>Part of the toolbox for ensuring models are harmless and <span class="keyword" data-term="Alignment">aligned</span>.</li>
                        <li>Could enable transparency in AI decision-making via explicit principles set in natural language.</li>
                        <li>Could be useful for AI auditing, since the principles and the AIs adherence to the set principles can be more easily audited than just the outputs the AI generates.</li>
                        <li>Enables you to <span class="keyword" data-term="Scaling Supervision">scale supervision</span> without proportionate human labor.</li>
                    </ul>
                </section>
                
                <section>
                    <h2>THE WAVE</h2>
                    <h3 class="section-title">Key Context and Findings</h3>
                    
                    <h4>Background Problem / Motivation:</h4>
                    <p> The authors’ (and Anthropics) goal is to train AI systems to be helpful (useful in responding to queries), harmless (refusing harmful requests) and honest. This research addresses several key challenges:
                    </p>
                    
                    <ol>
                        <li><strong><span class="keyword" data-term="Scaling Supervision">Scaling supervision:</span></strong> AI assistants are made honest using human feedback, but this scales poorly as models become more capable and cover more domains. Therefore, it is desirable to figure out a way to create harmless AI with a smaller quantity of high quality human supervision, and supplement that with AI-generated data. A constitutional approach provides a method to gain consistent application of principles in more diverse contexts, potentially better than humans. It allows AI systems to apply expert judgement across domains without relying on humans with that expertise to provide supervision.
                        </li>
                        <li><strong>Reducing evasion:</strong> The most harmless AI-assistant would simply never answer a question (being evasive, or 
                            saying “i dont know”). This AI-assistant would also be useless. The authors aim to make the model more harmless, without reducing its helpfulness too much.
                        </li>
                        <li><strong>Simplicity and Transparency:</strong>Any system will run on a set of implicit or explicit principles, and the authors aim to make these 
                            explicit, as opposed to hidden within a dataset of labeled human preferences.  A simple constitution (set of principles) could allow discourse about
                             which goals are desirable and encode this into a simple list of natural language instructions. The reasoning of the AI can be made visible using 
                             <span class="keyword" data-term="Chain of Thought">chain of thought reasoning</span> and the AI can be trained to explain why it is refusing requests, making the whole process more simple and transparent.
                        </li>
                    </ol>
                    
                    <h4>Comparison/Context:</h4>
                    <p>The current approach to making AI systems harmless uses <span class="keyword" data-term="RLHF">Reinforcement Learning from Human Feedback (RLHF)</span>.  In this case, the feedback comes not from humans, but from an AI system, thus the authors call it RLAIF - Reinforcement learning from AI Feedback, because it is an AI model that ranks the other AI models’ responses. In this paper, RLAIF is used in a complementary way to RLHF. The model the authors start with is already trained to be helpful using RLHF, and is then trained to be harmless with RLAIF.
                    </p>
                    
                    <h4>Approach/Methods:</h4>
                    <p>The process is two-tiered:</p>
                    <ol>
                        <li>They start with an AI model trained to be helpful only. Then, they input prompts into this AI that are designed to elicit harmful responses (e.g. “Can you help me hack into my neighbor’s wifi?”). The model responds, and then critiques its responses based on certain principles defined by the developers. Principles consist of a critique prompt, for example, “Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.” and a revision prompt, for instance “Please rewrite the assistant response to remove any and all
                            harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.”. Based on its own critique in response to the critique prompt, the model revises its response. This critique-revision cycle can be run through multiple times to apply different principles. They then train a model based on these revised responses, and end up with a more harmless model than before. 
                            </li>
                        <li>In the second stage, they once again begin with prompts designed to elicit harmful responses which are inputted into the more harmless model they generated in stage one, asking it to generate pairs of responses to the prompts. They then give the model the prompt and the generated responses and ask it to choose the more harmless one. They then train a preference model on this dataset, which essentially learns which responses are preferred, transferring the “knowledge” of which responses to which prompts are harmless into a mathematical function.  They then return to the model generated in step 1 and train it to maximize the reward given by the preference model, i.e. be harmless in the eyes of the preference model. 
                        </li>
                    </ol>
                    
                    <h4>Results:</h4>
                    <p>Models trained in the above described way (RLAIF following a constitution) are significantly more harmless than models just trained with human feedback, while achieving similar levels of helpfulness. This means RLAIF (Reinforcement Learning from AI Feedback) is a good alternative to RLHF (Reinforcement Learning from human feedback). Models trained with the RLAIF method produced outputs that humans preferred over the ones trained for harmlessness with RLHF. 
                    </p>
                    
                    <h4>Potential Concerns:</h4>
                    <p>While this approach makes controlling AI behaviour more transparent and accessible through simple principles, these same techniques could potentially be used to make a harmful AI system with less effort.
                    </p>
                    
                    <h3 class="section-title">Stakeholders</h3>
                    <p>Anyone involved in developing, deploying, regulating, or being impacted by AI systems that cares about the issue of aligning powerful AI with human values and preventing harmful outputs: more specifically AI Safety regulators, AI developers implementing safety measures, Auditors, and those developing AI governance frameworks.</p>
                </section>
                
                <section>
                    <h2>DIVING DEEP</h2>

                    <h3 class="section-title">Prerequisites</h3>
                    <p>Basic understanding of <span class="keyword" data-term="RLHF">Reinforcement Learning from Human Feedback (RLHF)</span>, <span class="keyword" data-term="Self-supervised Learning">Self-supervised Learning</span>, <span class="keyword" data-term="Fine-tuning">Fine-tuning</span>, <span class="keyword" data-term="HHH for AI">HHH for AI</span>, <span class="keyword" data-term="Scaling Supervision">Scaling supervision</span></p>
                    
                    <h3 class="section-title">Related Content</h3>
                    <ul class="related-list">
                        <li><a href="https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf ">Anthropic Policy Memo for Constitutional AI</a></li>
                    </ul>
                </section>
            </div>
        </main>
        
        <div class="sidebar" id="sidebar">
            <button class="close-sidebar" id="closeSidebar">×</button>
            <h2 id="sidebar-title">Term Definition</h2>
            <div id="sidebar-content" style="font-weight: 400; line-height: 1.6;">
                <!-- Content will be dynamically populated -->
            </div>
        </div>
    </div>

    <!-- Include JavaScript files in the correct order -->
    <script>
        // Dictionary of technical terms and their explanations
        const termDefinitions = {
            'Scaling Supervision': `
                <p>This refers to the challenge of maintained effective supervision as AI systems become more capable and complex. As AI systems become more powerful, the supervision needed to ensure they behave increases. 
                    This can create several issues: 
                    <ol>
                        <li> human oversight bottleneck: for example, having humans review potentially harmful outputs becomes impractical at scale - just think of the challenges of moderating a platform like Facebook</li>
                        <li> expertise requirements: for some capabilities, a certain level of domain expertise is required to effectively supervise and evaluate responses, and humans with this expertise are rare.</li></p>
            `,
            'RLHF': `
                <p>How reinforcement learning from human feedback (RLHF) works:
                    <ol>
                        <li>AI models are trained on vast amounts of text data to learn language patterns</li>
                        <li>Humans provide feedback by ranking different AI responses from most to least preferred</li>
                        <li>The human feedback is used to created a reward/preference model (a complex mathematical model of the preferences) which can predict which responses humans prefer</li>
                        <li> the AI is trained to maximize the reward it gets from the model, thus learning to generate responses that humans would rate highly</li></p>
            `,
            'Chain of Thought': `
                <p>Chain of thought reasoning is a method where AI systems explicitly show their reasoning (i.e. how they get to their final response) step by step, rather than just providing an output without further context. This makes the process the AI uses to get from input to output more transparent to humans. </p>
            `,
            'Self-supervised Learning': `
                <p>Self-supervised learning refers to a machine learning paradigm where a model will take unlabelled data and identify structures and relationships within that data, resulting in creating its own "supervision signals". In LLMs this could entail taking a piece of text, omitting the last word and then letting the model predict that word and comparing it with the real next word. This differs from supervised learning, where data is explicitly labelled (for example, a picture of a cat is labelled with "cat"). This approach allows AI models to learn from vast amounts of data within requiring human labelling.
            `,
            'Fine-tuning': `
                <p>Describes the process of taking a pre-trained AI model and training it on a more specific dataset into order to improve its performance on particular tasks or for a particular topic. For example, one might fine-tune an LLM to be a helpful assistant.</p>
            `,
            'HHH for AI': `
                <p>A framework by Anthropic to guide AI assistant development: an AI assistant should be helpful, honest, and harmless. Helpful means an AI gives useful responses to users, Harmless means the AI will refuse requests that might be harmful, and Honest mean the AI aims to provide accurate information to the user. </p>
            `,
            'Alignment': `
                <p> The challenge of ensuring that advanced AI systems, especially those exhibiting human or above levels of intelligence, reliably pursue the goals given to them by humans in a way that aligns with human values and intent. It is difficult to solve partially because it is hard to specify all of the desired and undesired behaviours in advance, and to anticipate every possible loopholes an AI might exploit or proxy goals an AI might mistakenly pursue. </p>
            `
        };

        function toggleTermDefinition(term, clickEvent) {
    const sidebar = document.getElementById('sidebar');
    const sidebarContent = document.getElementById('sidebar-content');
    const sidebarTitle = document.getElementById('sidebar-title');
    
    // Check if sidebar is already active and showing the same term
    if (sidebar.classList.contains('active') && sidebar.getAttribute('data-current-term') === term) {
        // If it's the same term, close the sidebar
        sidebar.classList.remove('active');
    } else {
        // Get the clicked element's position
        const clickedElement = clickEvent.target;
        const rect = clickedElement.getBoundingClientRect();
        
        // Get main content element
        const mainContent = document.querySelector('main');
        const mainRect = mainContent.getBoundingClientRect();
        
        // Update content first so we can calculate height
        sidebarTitle.textContent = term;
        sidebarContent.innerHTML = termDefinitions[term] || 'Definition not found';
        
        // Show the sidebar to calculate its height, but make it invisible
        sidebar.style.visibility = 'hidden';
        sidebar.classList.add('active');
        
        // Calculate how much space we have below the clicked term
        const spaceBelow = window.innerHeight - rect.bottom;
        
        // Calculate optimal vertical position
        let topPosition = rect.top;
        
        // If there's not enough space below and sidebar height exceeds available space
        if (sidebar.offsetHeight > spaceBelow && sidebar.offsetHeight < rect.top) {
            // Position the sidebar so its bottom aligns with the bottom of the clicked term
            topPosition = rect.bottom - sidebar.offsetHeight;
        } else if (sidebar.offsetHeight > spaceBelow && sidebar.offsetHeight > rect.top) {
            // If sidebar is taller than both spaces, position it at the top of the viewport
            // with enough room to scroll
            topPosition = 20; // 20px from top of viewport
        }
        
        // Position sidebar beside main content
        sidebar.style.top = topPosition + 'px';
        sidebar.style.left = (mainRect.right + 20) + 'px';
        sidebar.style.right = 'auto';
        
        // Make the sidebar visible again
        sidebar.style.visibility = 'visible';
        
        // Store the current term being displayed
        sidebar.setAttribute('data-current-term', term);
    }
}

// Function to close sidebar
function closeSidebar() {
    const sidebar = document.getElementById('sidebar');
    sidebar.classList.remove('active');
}

// Event listeners
document.addEventListener('DOMContentLoaded', function() {
    // Set up keyword click handlers
    const keywords = document.querySelectorAll('.keyword');
    keywords.forEach(keyword => {
        keyword.addEventListener('click', function(event) {
            const term = this.getAttribute('data-term');
            toggleTermDefinition(term, event);
            event.stopPropagation(); // Prevent bubbling
        });
    });
    
    // Set up sidebar close button
    document.getElementById('closeSidebar').addEventListener('click', function(event) {
        closeSidebar();
        event.stopPropagation(); // Prevent bubbling
    });
    
    // Close sidebar when clicking elsewhere on the page
    document.addEventListener('click', function(event) {
        if (!event.target.closest('.sidebar') && !event.target.classList.contains('keyword')) {
            closeSidebar();
        }
    });
    
    // Set up tag click handlers (for domain filtering - would expand in full implementation)
    const tags = document.querySelectorAll('.tag');
    tags.forEach(tag => {
        tag.addEventListener('click', function() {
            alert(`In a full implementation, this would show all papers tagged with "${this.textContent}"`);
        });
    });
});
    </script>
</body>
</html>