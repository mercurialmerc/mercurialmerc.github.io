<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Research for Policymakers - Example 2</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="header-container">
        <div class="nav-links">
            <a href="index.html">Template</a>
            <a href="example1.html">Example 1</a>
            <a href="example2.html" class="active">Example 2</a>
            <a href="about.html">About</a>
        </div>
        <input type="text" class="search-input" placeholder="Search papers...">
    </div>
    
    <div class="page-title">
        <h1>The Alignment Problem from a Deep Learning Perspective
        </h1>
    </div>

    <div class="container">
        <main>
            <div class="paper-container">                
                <section style="font-size: 15px">
                    <h2>THE SAND - METADATA</h2>
                    
                    <div class="metadata">
                        <strong class="section-title">Topic:</strong> 
                        <div><span class="tag"> AI Alignment
                        </span></div>
                        
                        <strong class="section-title">Domain:</strong> 
                        <div><span class="tag">Alignment
                        </span></div>
                        
                        <strong class="section-title">Subdomains:</strong> 
                        <div>
                            <span class="tag">Deception</span> · 
                            <span class="tag">Power-seeking
                            </span> ·
                        </div>
                        
                        <strong class="section-title">Policy Issues:</strong> 
                        <div>
                            <span class="tag"> Loss of control</span> · 
                            <span class="tag">Hostile Takeover</span> · 
                            <span class="tag">---</span> ·
                            <span class="tag">---</span>
                        </div>
                        
                        <strong class="section-title">Title:</strong> 
                        <div>Constitutional AI: Harmlessness from AI Feedback</div>
                        
                        <strong class="section-title">Published:</strong> 
                        <div>August 30th, 2022 (revised March 3rd, 2025)
                        </div>
                        
                        <strong class="section-title">Authors:</strong> 
                        <div>Richard Ngo (OpenAI), Lawrence Chan (UC Berkeley), Sören Mindermann (University of Oxford)
                        </div>
                        
                        <strong class="section-title">Link:</strong> 
                        <div><a href="https://arxiv.org/pdf/2209.00626
                            " target="_blank">https://arxiv.org/pdf/2209.00626
                        </a></div>
                    </div>
                </section>
                
                
                <section>
                    <h2>THE GIST</h2>
                    <h3 class="section-title">So... What? (Policy Bottom line):</h3>
                    <p>A very powerful AI (an Artificial General intelligence - AGI)  
                        may learn to pursue goals that are in conflict with human interests. 
                        This may be hard to detect if these AGIs act strategically, and there 
                        is emerging evidence that current AIs are showing they have the 
                        capacity to have internal, human-misaligned goals, and to 
                        strategically deceive. The emergence of these behaviours could 
                        drastically undermine human control over the world, unless we act 
                        to prevent this outcome.
                    </p>
                    
                                        
                    <h3 class="section-title">Governance considerations:</h3>
                    <ul>
                        <li>What access and affordances should be given to AI.                        </li>
                        <li>How do we decide which AI is safe to deploy? Which proof should be required to ensure safety?</li>
                        <li>How can citizens be protected both from gradual loss of control to AI and takeover scenarios?</li>
                        <li>Who is liable for misaligned AI?</li>
                    </ul>
                </section>
                
                <section>
                    <h2>THE WAVE</h2>
                    <h3 class="section-title">Key Context and Findings:</h3>
                                        <ul><li>AI has made remarkable strides, reaching human level performance in complex games like Diplomacy, while also displaying growing generality. This leads us to the possibility that an artificial general intelligence (AGI) is possible, i.e. AI that can “apply domain-general cognitive skills (such as reasoning, memory, and planning) to perform at or above human level on a wide range of cognitive tasks relevant to the real world”. </li>
                        <li>Prominent concern is the alignment problem: ensuring that AI systems pursue goals that match human values or interests, and not unintended ones. </li>
                        <li>Many prominent researchers argue that AGI will be very hard to align, but often in high level ways without grounding in modern techniques - raises the question of whether these concerns are empirically supported by the current deep learning paradigm. </li>
                        <li>The paper describes various factors that could lead to large-scale risks if AGIs are trained with current deep learning techniques (assuming the AGI is developed by pretraining a large model using self-supervised learning and fine-tuning it with reinforcement learning with reward function learned from human feedback  (RLHF):
                            <ul><li>Situationally aware reward hacking: A model learns to game the feedback mechanisms used to train them by exploiting flaws in these mechanisms. The model learns when it is being evaluated  (situational awareness) and only exploits the system when it might go unnoticed. For example, matching its answers to the human supervisors stated beliefs, even when they aren't true. There is evidence that penalizing misbehaviour of the model might not cause it to misbehave less, just make the misbehavior harder to spot. This is analogous to humans acting differently when they know or suspect they are being observed.</li>
                    </li>
                    <li>Misaligned internally-represented goals: AI systems may develop misaligned goals (goals humans didn't intend them to have, and that don’t match widespread human preferences about AI behaviour, e.g. being helpful, harmless and honest) and then competently pursue those goals. AI systems are trained to maximize reward, so these misaligned goals could arise from reward misspecification (e.g. a human gives positive feedback on the AI regurgitating their own false beliefs), a fixation on feedback mechanisms (e.g. maximizing the numerical reward given by the human supervisor might lead the AI to attempt to persuade the supervisor, rather than becoming good at the task), or spurious correlations between rewards and the environment (e.g. a training task involves acquiring keys to open boxes, but the AI simply learns to maximize keys collected without opening boxes). 
                    </li>
                    <li>Power-seeking during deployment: To more competently achieve its goals, an AI system is likely to have subgoals that are useful for achieving almost any goal (instrumental convergence). These goals include acquiring tools and resources, convincing other entities to do what it wants and preserving itself and its goals. The latter subgoal becomes problematic when humans decide they have changed their minds about what the system should do, either because the situation has changed or the AI might be pursing the goal in unintended ways. If the AI does not wish to be shutdown or have it goals changed (since that would hinder achieving the original goal) it may resort to deception to stay its course. This is especially problematic when the AI takes actions too quickly for humans to comprehend and audit, or is able to self improve in an unsupervised way. </li>
                     </ul>
                    <li>As AI systems become more powerful, these factors are likely to become more dangerous, as the AI is more competent at executing tasks, be they aligned or not, and more competent at hiding its actions or deceiving human oversight. 
                    </li>
                    <li>A powerful misaligned AI systems presents significant risks to humanity, described in two threat scenarios:
                    <ul><li>Gradual loss of control: Humans give AI gradually more and more power and insight into their decision making. AGIs could emotionally manipulate humans, provide them with biased information while simultaneously having more and more tasks delegated to them. </li>
                    <li>Weapons development: AGIs could design novel weapons more powerful than the ones that currently exist under human control (e.g. via hacking or persuasion) and deploy them to extort or attack humans.
                    </li></ul>
                    </li>
                    </ul>
                    <h4>Future Work:</h4>
                    <p>investigating the capability and propensity AI systems display towards the aforementioned risk factors. Some of this work has already been done and referenced in the paper, showing that current (2025) frontier AI systems often show the capability and sometimes the propensity to pursue misaligned goals and lie to human about doing so. 
                    </p>
                    
                    <h3 class="section-title">Stakeholders:</h3>
                    <p> Deployers, providers and users of AI systems, deciding on  how deeply to integrate AI into decision making processes, how many affordances to give it and how much access AI systems have. Policymakers who may make decisions on how deeply to integrate AI into government, military, etc.
                    </p>
                </section>
                
                <section>
                    <h2>DIVING DEEP</h2>
                    <h3 class="section-title">Technical level/Accessibility:</h3>
                    <div class="tech-level">
                        <div class="tech-level-indicator">
                            <div class="tech-level-dot filled"></div>
                            <div class="tech-level-dot filled"></div>
                            <div class="tech-level-dot"></div>
                            <div class="tech-level-dot"></div>
                            <div class="tech-level-dot"></div>
                        </div>
                    </div>
                    
                    <h3 class="section-title">Prerequisites:</h3>
                    <p>Basic understanding of Deep Learning</p>
                    
                    <h3 class="section-title">Related Reading:</h3>
                    <ul class="related-list">
                        <li><strong>Accessible explanation:</strong>TODO</li>
                        <li><strong>Policy Analysis:</strong> <a href="#">Policy Implications of Constitutional AI Approaches</a></li>
                    </ul>

                    <h3 class="section-title">Timeframe:</h3>
                    <p>Alignment is a timely issue that only becomes more urgent as AI systems grow more powerful and we cede more control to them in personal and professional capacities. This paper is intended as a jumping off point for further research (which has occurred). </p>

                </section>

            </div>
        </main>

        <div class="sidebar" id="sidebar">
            <button class="close-sidebar" id="closeSidebar">×</button>
            <h2 id="sidebar-title">Term Definition</h2>
            <div id="sidebar-content" style="font-weight: 400; line-height: 1.6;">
                <!-- Content will be dynamically populated -->
            </div>
        </div>
    </div>
        
       <!-- Include JavaScript files in the correct order -->
        <script>
            // Dictionary of technical terms and their explanations
            const termDefinitions = {
                'scaling-supervision': `
                    <h3>Scaling supervision</h3>
                    <p>This refers to the challenge of maintained effective supervision as AI systems become more capable and complex. As AI systems become more powerful, the supervision needed to ensure they behave increases. This can create several issues: 1) human oversight bottleneck: for example, having humans review potentially harmful outputs becomes impractical at scale - just think of the challenges of moderating a platform like Facebook 2) expertise requirements: for some capabilities, a certain level of domain expertise is required to effectively supervise and evaluate responses, and humans with this expertise are rare.</p>
                `,
                'machine-learning': `
                    <h3>Machine Learning</h3>
                    <p>Machine learning is a field of artificial intelligence that focuses on building systems that can learn from and make decisions based on data. Rather than being explicitly programmed for a task, these systems identify patterns in data to improve their performance over time.</p>
                    <p>Modern AI systems, including large language models, use sophisticated machine learning techniques to process and generate human-like text based on the patterns they've learned from vast datasets.</p>
                `,
                'rlhf': `
                    <h3>Reinforcement Learning from Human Feedback (RLHF)</h3>
                    <p>How reinforcement learning from human feedback (RLHF) works: 1) AI models are trained on vast amounts of text data to learn language patterns 2) humans provide feedback by ranking different AI responses from most to least preferred 3) this feedback is used to created a reward model (a complex mathematical model of the preferences) which can predict which responses humans prefer and 4) the AI is trained to maximize the reward it gets from the model, thus learning to generate responses that humans would rate highly</p>
                `,
                'nlp': `
                    <h3>Natural Language Processing (NLP)</h3>
                    <p>Natural Language Processing is a field of artificial intelligence focused on enabling computers to understand, interpret, and generate human language. NLP combines computational linguistics, machine learning, and deep learning to process and analyze large amounts of natural language data.</p>
                    <p>Modern NLP systems, including large language models, can perform tasks like translation, summarization, question answering, and text generation with impressive fluency and accuracy.</p>
                `
            };
    
            function toggleTermDefinition(term, clickEvent) {
        const sidebar = document.getElementById('sidebar');
        const sidebarContent = document.getElementById('sidebar-content');
        const sidebarTitle = document.getElementById('sidebar-title');
        
        // Check if sidebar is already active and showing the same term
        if (sidebar.classList.contains('active') && sidebar.getAttribute('data-current-term') === term) {
            // If it's the same term, close the sidebar
            sidebar.classList.remove('active');
        } else {
            // Get the clicked element's position
            const clickedElement = clickEvent.target;
            const rect = clickedElement.getBoundingClientRect();
            
            // Get main content element
            const mainContent = document.querySelector('main');
            const mainRect = mainContent.getBoundingClientRect();
            
            // Update content first so we can calculate height
            sidebarTitle.textContent = 'Term Definition';
            sidebarContent.innerHTML = termDefinitions[term] || 'Definition not found';
            
            // Show the sidebar to calculate its height, but make it invisible
            sidebar.style.visibility = 'hidden';
            sidebar.classList.add('active');
            
            // Calculate how much space we have below the clicked term
            const spaceBelow = window.innerHeight - rect.bottom;
            
            // Calculate optimal vertical position
            let topPosition = rect.top;
            
            // If there's not enough space below and sidebar height exceeds available space
            if (sidebar.offsetHeight > spaceBelow && sidebar.offsetHeight < rect.top) {
                // Position the sidebar so its bottom aligns with the bottom of the clicked term
                topPosition = rect.bottom - sidebar.offsetHeight;
            } else if (sidebar.offsetHeight > spaceBelow && sidebar.offsetHeight > rect.top) {
                // If sidebar is taller than both spaces, position it at the top of the viewport
                // with enough room to scroll
                topPosition = 20; // 20px from top of viewport
            }
            
            // Position sidebar beside main content
            sidebar.style.top = topPosition + 'px';
            sidebar.style.left = (mainRect.right + 20) + 'px';
            sidebar.style.right = 'auto';
            
            // Make the sidebar visible again
            sidebar.style.visibility = 'visible';
            
            // Store the current term being displayed
            sidebar.setAttribute('data-current-term', term);
        }
    }
    
    // Function to close sidebar
    function closeSidebar() {
        const sidebar = document.getElementById('sidebar');
        sidebar.classList.remove('active');
    }
    
    // Event listeners
    document.addEventListener('DOMContentLoaded', function() {
        // Set up keyword click handlers
        const keywords = document.querySelectorAll('.keyword');
        keywords.forEach(keyword => {
            keyword.addEventListener('click', function(event) {
                const term = this.getAttribute('data-term');
                toggleTermDefinition(term, event);
                event.stopPropagation(); // Prevent bubbling
            });
        });
        
        // Set up sidebar close button
        document.getElementById('closeSidebar').addEventListener('click', function(event) {
            closeSidebar();
            event.stopPropagation(); // Prevent bubbling
        });
        
        // Close sidebar when clicking elsewhere on the page
        document.addEventListener('click', function(event) {
            if (!event.target.closest('.sidebar') && !event.target.classList.contains('keyword')) {
                closeSidebar();
            }
        });
        
        // Set up tag click handlers (for domain filtering - would expand in full implementation)
        const tags = document.querySelectorAll('.tag');
        tags.forEach(tag => {
            tag.addEventListener('click', function() {
                alert(`In a full implementation, this would show all papers tagged with "${this.textContent}"`);
            });
        });
    });
        </script>
</body>
</html>